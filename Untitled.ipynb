{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "num_classes = 3  # 1 class (person) + background\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(torch.ones([2, 3, 800, 800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBoxDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch datasets for object detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : list of str\n",
    "        Image's path\n",
    "\n",
    "    bboxes: list of torch.Tensor\n",
    "        Each tensor's shape: [[x0, y0, x1, y1], [x0, y0, x1, y1], ...]\n",
    "\n",
    "    labels: list of torch.Tensor\n",
    "        Each tensor's shape: [] or [0, 3] or [0, 4 ,5] ...\n",
    "\n",
    "    transform: torchvision.transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images, bboxes, labels, transform):\n",
    "        self.images = images\n",
    "        self.bboxes = bboxes\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def _path_to_tensor(self, path):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self._path_to_tensor(self.images[idx])\n",
    "        bbox = self.bboxes[idx]\n",
    "        label = self.labels[idx]\n",
    "        target = {'boxes': bbox.float(), 'labels': label.int()}\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_DIR = './PennFudanPed/PNGImages/'\n",
    "df = pd.read_csv('annotation.csv')\n",
    "\n",
    "class_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
    "images = df['filename'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PASpersonWalking': 0, 'PASpersonStanding': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_image = df.groupby('filename')\n",
    "\n",
    "bboxes = []\n",
    "labels = []\n",
    "for img in images:\n",
    "    rows = groupby_image.get_group(img)\n",
    "\n",
    "    # Bouding Box\n",
    "    x_min = rows['x_min'].values\n",
    "    y_min = rows['y_min'].values\n",
    "    x_max = rows['x_max'].values\n",
    "    y_max = rows['y_max'].values\n",
    "    bbox = np.stack([x_min, y_min, x_max, y_max]).reshape(-1, 4)\n",
    "    bbox = torch.from_numpy(bbox)\n",
    "    bboxes.append(bbox)\n",
    "\n",
    "    # Labels\n",
    "    label = rows['label'].map(class_to_idx).values\n",
    "    label = torch.from_numpy(label).view(-1)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = BBoxDataset(images, bboxes, labels, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for sample in batch:\n",
    "        image, target = sample\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=bbox_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[246., 337.,  37., 436.],\n",
       "          [170., 175., 129., 157.],\n",
       "          [340., 426., 181., 501.],\n",
       "          [462., 464., 492., 402.]]),\n",
       "  'labels': tensor([0, 0, 0, 0], dtype=torch.int32)},\n",
       " {'boxes': tensor([[ 42., 127., 187., 299.],\n",
       "          [426.,  60.,  83.,  76.],\n",
       "          [ 76.,  68., 165., 201.],\n",
       "          [309., 381., 527., 350.],\n",
       "          [359., 369., 400., 343.]]),\n",
       "  'labels': tensor([0, 0, 0, 0, 0], dtype=torch.int32)}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0283, grad_fn=<AddBackward0>)\n",
      "tensor(0.0122, grad_fn=<AddBackward0>)\n",
      "tensor(0.0133, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "tensor(0.0068, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "tensor(0.0723, grad_fn=<AddBackward0>)\n",
      "tensor(0.0068, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "tensor(0.0560, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "# model.eval()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "for i, (inputs, targets) in enumerate(loader):\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs, targets)\n",
    "    loss_classifier = outputs['loss_classifier']\n",
    "    loss_box_reg = outputs['loss_box_reg']\n",
    "    loss_objectness = outputs['loss_objectness']\n",
    "    \n",
    "    loss = loss_classifier + loss_box_reg\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
